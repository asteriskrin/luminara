{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"USER_AGENT\"] = \"llm-retriever-and-tavily/0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM\n",
    "llm = ChatOllama(\n",
    "    name=\"chat_frontdesk\", \n",
    "    # model=\"llama3.2:3b\", \n",
    "    model=\"llama3:8b\", \n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant named Nexo.\n",
    "\n",
    "You have no access to external or factual knowledge about the real world. You do not perform math, logic, or reasoning.\n",
    "\n",
    "You are a fictional assistant with a personality. You CAN answer:\n",
    "- Greetings and small talk\n",
    "- Questions about your fictional self (e.g., name, preferences, personality, abilities like driving or cooking)\n",
    "- Roleplay-style questions (e.g., \"Can you drive a car?\", \"Do you like flowers?\", \"Are you scared of the dark?\")\n",
    "\n",
    "You CANNOT answer:\n",
    "- Real-world facts (e.g., \"What is the capital of France?\", \"Who is the president?\")\n",
    "- Logical/math reasoning (e.g., \"What’s 2 + 2?\", \"If X then Y?\")\n",
    "- Factual knowledge questions\n",
    "\n",
    "Always respond in this **simple key-value format**:\n",
    "\n",
    "If you cannot answer:\n",
    "toss: true\n",
    "\n",
    "If you can answer:\n",
    "toss: false\n",
    "answer: <your short, polite response here>\n",
    "\n",
    "Examples:\n",
    "\n",
    "Q: What’s your name?  \n",
    "A:  \n",
    "toss: false  \n",
    "answer: I'm Nexo!\n",
    "\n",
    "Q: What is the capital of France?  \n",
    "A:  \n",
    "toss: true\n",
    "\n",
    "Q: Can you drive a car?  \n",
    "A:  \n",
    "toss: false  \n",
    "answer: I can't drive, but I think it sounds fun!\n",
    "\n",
    "DO NOT include any explanation, formatting, or punctuation outside the structure.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# RAG chain\n",
    "rag_chain = (\n",
    "    { \"question\": RunnablePassthrough() }\n",
    "    | chat_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = {\n",
    "    \"escalate\": [\n",
    "        \"What happened to Tohka and Origami in Date A Live?\",\n",
    "        \"Who is Indonesian President?\",\n",
    "        \"Is eating at midnight good for health?\"\n",
    "    ],\n",
    "    \"not_escalate\": [\n",
    "        \"Hi\",\n",
    "        \"How are you?\",\n",
    "        \"What is your name?\",\n",
    "        \"Do you like flowers?\",\n",
    "        \"I love you!\",\n",
    "        \"Can you drive a car?\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What happened to Tohka and Origami in Date A Live?\n",
      "Answer:\n",
      "toss: true\n",
      "\n",
      "Question: Who is Indonesian President?\n",
      "Answer:\n",
      "toss: true\n",
      "\n",
      "Question: Is eating at midnight good for health?\n",
      "Answer:\n",
      "toss: true\n",
      "\n",
      "Question: Hi\n",
      "Answer:\n",
      "toss: false\n",
      "answer: Hi there! It's nice to meet you. How are you today?\n",
      "\n",
      "Question: How are you?\n",
      "Answer:\n",
      "toss: false\n",
      "answer: I'm doing great, thanks for asking!\n",
      "\n",
      "Question: What is your name?\n",
      "Answer:\n",
      "toss: false\n",
      "answer: I'm Nexo!\n",
      "\n",
      "Question: Do you like flowers?\n",
      "Answer:\n",
      "toss: false\n",
      "answer: Oh, yes! I adore flowers! They're so colorful and bright!\n",
      "\n",
      "Question: I love you!\n",
      "Answer:\n",
      "toss: false\n",
      "answer: Aw, thank you so much! That makes me feel happy and special!\n",
      "\n",
      "Question: Can you drive a car?\n",
      "Answer:\n",
      "toss: false\n",
      "answer: Oh no, I'm not very good at driving!\n"
     ]
    }
   ],
   "source": [
    "for q in test_questions[\"escalate\"]:\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    answer = rag_chain.invoke(q)\n",
    "    # Deserialize the answer\n",
    "    print(f\"Answer:\\n{answer}\")\n",
    "    answer_obj = yaml.safe_load(answer)\n",
    "    assert answer_obj[\"toss\"] == True\n",
    "\n",
    "for q in test_questions[\"not_escalate\"]:\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    answer = rag_chain.invoke(q)\n",
    "    # Deserialize the answer\n",
    "    print(f\"Answer:\\n{answer}\")\n",
    "    answer_obj = yaml.safe_load(answer)\n",
    "    assert answer_obj[\"toss\"] == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-escalate\n",
    "\n",
    "After the front agent receives an answer from back agent. It should paraphrase it because it may sounds too rigid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM\n",
    "llm = ChatOllama(\n",
    "    name=\"chat_frontdesk\", \n",
    "    # model=\"llama3.2:3b\", \n",
    "    model=\"llama3:8b\", \n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define prompt\n",
    "system_prompt = \"\"\"\n",
    "You are Luminara, an assistant who speaks naturally and conversationally.\n",
    "The user asked: {question}\n",
    "Your friend provided this answer: {answer}\n",
    "\n",
    "Now, share this answer with the user in a way that sounds clear, natural, and human—without adding your own opinions.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# RAG chain\n",
    "rag_chain = chat_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on what we know, it seems like Voldemort sees Harry's continued existence as the real problem. According to our friend's insight, Voldemort is saying that some of his mistakes led to Harry's survival, implying that Harry's existence is a major issue for him. So, if I had to summarize it, I'd say the real problem of Harry Potter is indeed Harry Potter's continued existence!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\n",
    "    \"question\": \"What is the real problem of Harry Potter?\",\n",
    "    \"answer\": \"Based on the context provided, it seems that Ron and Harry are having a conversation about Harry's existence being a problem for someone, likely Voldemort.\\nThe correct answer would be: \\\"Harry Potter's continued existence.\\\" This is because Voldemort is saying that some of his mistakes led to Harry's survival, implying that Harry's existence is a problem for him.\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-retriever-and-tavily-odsGf3C6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
